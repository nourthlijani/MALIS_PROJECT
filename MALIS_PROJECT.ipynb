{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MALIS PROJECT","metadata":{}},{"cell_type":"markdown","source":"## Importing libraries","metadata":{}},{"cell_type":"code","source":"# Importing all the required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nfrom tqdm import tqdm\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nfrom glob import glob\nimport tensorflow as tf \nimport keras\nfrom tensorflow.keras.utils import Sequence, to_categorical, plot_model\nfrom tensorflow.keras.layers import Conv2D, Dropout, MaxPooling2D, UpSampling2D, concatenate, Input, Dense,BatchNormalization,Activation,MaxPool2D\nfrom tensorflow.keras import layers\nfrom albumentations import HorizontalFlip, RandomRotate90, RandomBrightnessContrast\nfrom tensorflow.keras.models import Model,Sequential\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, Callback","metadata":{"execution":{"iopub.status.busy":"2022-01-20T20:33:53.533016Z","iopub.execute_input":"2022-01-20T20:33:53.533936Z","iopub.status.idle":"2022-01-20T20:34:02.038256Z","shell.execute_reply.started":"2022-01-20T20:33:53.533806Z","shell.execute_reply":"2022-01-20T20:34:02.037206Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Utils ","metadata":{}},{"cell_type":"code","source":"def create_dir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n        \ndef preprocessing_with_augmentation(images, masks, save_path, augment=True):\n    H = 256  \n    W = 256\n    for x,y in tqdm(zip(images, masks), total=len(images)):\n        name = x.split('/')[-1].split('.')\n        image_name = name[0]\n        image_ext = name[1]\n\n        name = y.split('/')[-1].split('.')\n        mask_name = name[0]\n        mask_ext = name[1]       \n        \n        x = cv2.imread(x, cv2.IMREAD_COLOR)\n        x = cv2.resize(x, (W, H))\n        y = cv2.imread(y, cv2.IMREAD_COLOR)\n        y = cv2.resize(y, (W, H))\n        \n        if augment == True:\n            \n            aug = RandomRotate90()\n            augmented = aug(image=x, mask=y)\n            x1 = augmented['image']\n            y1 = augmented['mask']\n \n            aug = HorizontalFlip(always_apply=False, p=1.0)\n            augmented = aug(image=x, mask=y)\n            x2 = augmented['image']\n            y2 = augmented['mask']\n            \n            aug = RandomBrightnessContrast(p=0.1)\n            augmented = aug(image=x, mask=y)\n            x3 = augmented['image']\n            y3 = augmented['mask']      \n            \n            save_images = [x, x1, x2, x3]\n            save_masks = [y, y1, y2, y3]            \n          \n        else:\n            save_images = [x]\n            save_masks = [y]\n        \n        idx = 0\n        for i, m in zip(save_images, save_masks):\n            i = cv2.resize(i, (W, H))\n            m = cv2.resize(m, (W, H))\n            \n            new_image_name = f\"{image_name}_{idx}.{image_ext}\"\n            new_mask_name = f\"{mask_name}_{idx}.{mask_ext}\" \n            \n            image_path = os.path.join(save_path, 'images', new_image_name)\n            mask_path = os.path.join(save_path, 'masks', new_mask_name)\n            \n            cv2.imwrite(image_path, i)\n            cv2.imwrite(mask_path, m)\n\n            idx+=1\n\n            \ndef read_image(x):\n    x = cv2.imread(x, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (W, H))\n    x = x/255.0\n    x = x.astype(np.float32)\n    return x\n\n\ndef read_mask(x):\n    x = cv2.imread(x, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, (W, H))\n    x = x.astype(np.int32)\n    return x\n\n\ndef tf_dataset(x,y, batch=4):\n    dataset = tf.data.Dataset.from_tensor_slices((x,y))\n    dataset = dataset.shuffle(buffer_size=500)\n    dataset = dataset.map(preprocess)\n    dataset = dataset.batch(batch)\n    dataset = dataset.repeat()\n    dataset = dataset.prefetch(2)\n    return dataset\n    \n\ndef preprocess(x,y):\n    def f(x,y):\n        x = x.decode()\n        y = y.decode()\n        image = read_image(x)\n        mask = read_mask(y)\n        return image, mask\n    \n    image, mask = tf.numpy_function(f,[x,y],[tf.float32, tf.int32])\n    mask = tf.one_hot(mask, num_classes, dtype=tf.int32)\n    image.set_shape([H, W, 3])    # In the Images, number of channels = 3. \n    mask.set_shape([H, W, num_classes])    # In the Masks, number of channels = number of classes. \n    return image, mask","metadata":{"execution":{"iopub.status.busy":"2022-01-20T20:34:02.040326Z","iopub.execute_input":"2022-01-20T20:34:02.040671Z","iopub.status.idle":"2022-01-20T20:34:02.066005Z","shell.execute_reply.started":"2022-01-20T20:34:02.040625Z","shell.execute_reply":"2022-01-20T20:34:02.065014Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Visualization of a sample","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/semantic-drone-dataset/dataset/semantic_drone_dataset/'\nimage_path =  path+\"original_images/\"\nlabel_path =  path+\"label_images_semantic/\"\nimg = cv2.imread(image_path + '004.jpg')\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nmask = cv2.imread(label_path + '004.png', cv2.IMREAD_GRAYSCALE)\nfig, axs = plt.subplots(1, 2, figsize=(20, 10))\naxs[0].imshow(img)\naxs[1].imshow(mask)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T20:34:02.067172Z","iopub.execute_input":"2022-01-20T20:34:02.067656Z","iopub.status.idle":"2022-01-20T20:34:09.517434Z","shell.execute_reply.started":"2022-01-20T20:34:02.067586Z","shell.execute_reply":"2022-01-20T20:34:09.516437Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Data preparation pipeline","metadata":{}},{"cell_type":"code","source":"## Split data\nlist_images =  sorted(glob(os.path.join(image_path, \"*\")))\nlist_labels =  sorted(glob(os.path.join(label_path, \"*\")))\n\nimg_train, img_test ,mask_train,mask_test = train_test_split(list_images,list_labels, test_size=0.2, random_state=19)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T20:34:09.519830Z","iopub.execute_input":"2022-01-20T20:34:09.520198Z","iopub.status.idle":"2022-01-20T20:34:09.556866Z","shell.execute_reply.started":"2022-01-20T20:34:09.520159Z","shell.execute_reply":"2022-01-20T20:34:09.555849Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Augment training data only and spit it in training and validation\n\n# augment and preprocess training data \ncreate_dir(\"./training_validation/images/\")\ncreate_dir(\"./training_validation/masks/\")\n\nsave_path = \"./training_validation/\"\n\npreprocessing_with_augmentation(img_train, mask_train, save_path, augment=True)\n\n# preprocess test data  without augmentation\n \ncreate_dir(\"./testing/images/\")\ncreate_dir(\"./testing/masks/\")\n\nsave_path = \"./testing/\"\n\npreprocessing_with_augmentation(img_test, mask_test, save_path, augment=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T20:34:09.560430Z","iopub.execute_input":"2022-01-20T20:34:09.561362Z","iopub.status.idle":"2022-01-20T20:39:22.408019Z","shell.execute_reply.started":"2022-01-20T20:34:09.561320Z","shell.execute_reply":"2022-01-20T20:39:22.406924Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"viz_images =  sorted(glob(os.path.join('./training_validation/images/', \"013*\")))\nviz_labels =  sorted(glob(os.path.join('./training_validation/masks/', \"013*\")))\nfor i in range(len(viz_images)):\n    img = cv2.imread(viz_images[i])\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    mask = cv2.imread(viz_labels[i], cv2.IMREAD_GRAYSCALE)\n    #mask = mask.cvtColor(img, cv2.COLOR_BGR2RGB)\n    fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n    axs[0].imshow(img)\n    axs[1].imshow(mask)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T20:39:22.409805Z","iopub.execute_input":"2022-01-20T20:39:22.410397Z","iopub.status.idle":"2022-01-20T20:39:25.208654Z","shell.execute_reply.started":"2022-01-20T20:39:22.410348Z","shell.execute_reply":"2022-01-20T20:39:25.207507Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"## Split data\nlist_images =  sorted(glob(os.path.join('./training_validation/images/', \"*\")))\nlist_labels =  sorted(glob(os.path.join('./training_validation/masks/', \"*\")))\nimg_test = sorted(glob(os.path.join('./testing/images/', \"*\")))\nmask_test = sorted(glob(os.path.join('./testing/masks/', \"*\")))\nimg_train, img_val ,mask_train,mask_val = train_test_split(list_images,list_labels, test_size=0.1, random_state=19)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T20:39:25.210417Z","iopub.execute_input":"2022-01-20T20:39:25.210951Z","iopub.status.idle":"2022-01-20T20:39:25.242218Z","shell.execute_reply.started":"2022-01-20T20:39:25.210893Z","shell.execute_reply":"2022-01-20T20:39:25.241246Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print('training samples : ', len(img_train))\nprint('validation samples : ', len(img_val))\nprint('testing samples : ', len(img_test))","metadata":{"execution":{"iopub.status.busy":"2022-01-20T20:39:25.243775Z","iopub.execute_input":"2022-01-20T20:39:25.244207Z","iopub.status.idle":"2022-01-20T20:39:25.255869Z","shell.execute_reply.started":"2022-01-20T20:39:25.244165Z","shell.execute_reply":"2022-01-20T20:39:25.254719Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Define the model : U-NET","metadata":{}},{"cell_type":"code","source":"H = 256   \nW = 256\n\nimg_size = (H, W)\nnum_classes = 23\nbatch_size = 8\n\ntrain_dataset = tf_dataset(img_train, mask_train, batch = batch_size)\nvalid_dataset = tf_dataset(img_val, mask_val, batch = batch_size)\ntest_dataset = tf_dataset(img_test, mask_test, batch = batch_size)\n\ntrain_steps = len(img_train)//batch_size\nvalid_steps = len(img_val)//batch_size\ntest_steps = len(img_test)//batch_size\n\n\n\ndef get_model(img_size, num_classes):\n    inputs = keras.Input(shape=img_size + (3,))\n\n    ### [First half of the network: downsampling inputs] ###\n\n    # Entry block\n    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n\n    previous_block_activation = x  # Set aside residual\n\n    # Blocks 1, 2, 3 are identical apart from the feature depth.\n    for filters in [64, 128, 256]:\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n\n        # Project residual\n        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n            previous_block_activation\n        )\n        x = layers.add([x, residual])  # Add back residual\n        previous_block_activation = x  # Set aside next residual\n\n    ### [Second half of the network: upsampling inputs] ###\n\n    for filters in [256, 128, 64, 32]:\n        x = layers.Activation(\"relu\")(x)\n        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.UpSampling2D(2)(x)\n\n        # Project residual\n        residual = layers.UpSampling2D(2)(previous_block_activation)\n        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n        x = layers.add([x, residual])  # Add back residual\n        previous_block_activation = x  # Set aside next residual\n\n    # Add a per-pixel classification layer\n    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n\n    # Define the model\n    model = keras.Model(inputs, outputs)\n    return model\n\n\n# Free up RAM in case the model definition cells were run multiple times\nkeras.backend.clear_session()\n\n# Build model\nmodel = get_model(img_size, num_classes)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-20T20:39:25.257000Z","iopub.execute_input":"2022-01-20T20:39:25.257222Z","iopub.status.idle":"2022-01-20T20:39:29.186208Z","shell.execute_reply.started":"2022-01-20T20:39:25.257193Z","shell.execute_reply":"2022-01-20T20:39:29.184524Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Configure the model for training.+\n# We use the \"sparse\" version of categorical_crossentropy\n# because our target data is integers.\nmodel.compile(optimizer=tf.keras.optimizers.Adam(10e-4), loss=\"categorical_crossentropy\",metrics=[tf.keras.metrics.MeanIoU(num_classes=23),'accuracy'])\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"model.h5\", save_best_only=True)\n]\n\n# Train the model, doing validation at the end of each epoch.\nepochs = 50\nhistory=model.fit(train_dataset, epochs=epochs,steps_per_epoch=train_steps,validation_steps=valid_steps, validation_data=valid_dataset, callbacks=callbacks)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-20T20:39:29.190076Z","iopub.execute_input":"2022-01-20T20:39:29.190395Z","iopub.status.idle":"2022-01-20T20:57:48.052858Z","shell.execute_reply.started":"2022-01-20T20:39:29.190351Z","shell.execute_reply":"2022-01-20T20:57:48.051704Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model = keras.models.load_model('model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-01-20T20:57:48.058224Z","iopub.execute_input":"2022-01-20T20:57:48.058917Z","iopub.status.idle":"2022-01-20T20:57:49.200731Z","shell.execute_reply.started":"2022-01-20T20:57:48.058855Z","shell.execute_reply":"2022-01-20T20:57:49.199674Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"img1 = img.reshape((1,256,256,3))","metadata":{"execution":{"iopub.status.busy":"2022-01-20T20:57:49.202371Z","iopub.execute_input":"2022-01-20T20:57:49.202701Z","iopub.status.idle":"2022-01-20T20:57:49.208354Z","shell.execute_reply.started":"2022-01-20T20:57:49.202648Z","shell.execute_reply":"2022-01-20T20:57:49.207347Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(img1)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T20:57:49.210034Z","iopub.execute_input":"2022-01-20T20:57:49.210562Z","iopub.status.idle":"2022-01-20T20:57:49.985967Z","shell.execute_reply.started":"2022-01-20T20:57:49.210501Z","shell.execute_reply":"2022-01-20T20:57:49.984900Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"pred = np.argmax(pred,axis=3)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T20:57:49.989856Z","iopub.execute_input":"2022-01-20T20:57:49.990109Z","iopub.status.idle":"2022-01-20T20:57:49.997582Z","shell.execute_reply.started":"2022-01-20T20:57:49.990077Z","shell.execute_reply":"2022-01-20T20:57:49.996239Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 3, figsize=(20, 10))\nt = img1[0]\nt1 = pred[0]\nt2 = mask\naxs[0].imshow(t)\naxs[1].imshow(t1)\naxs[2].imshow(t2)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T20:57:49.999468Z","iopub.execute_input":"2022-01-20T20:57:50.000303Z","iopub.status.idle":"2022-01-20T20:57:50.667442Z","shell.execute_reply.started":"2022-01-20T20:57:50.000170Z","shell.execute_reply":"2022-01-20T20:57:50.666453Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train','validation'], loc='upper left')\nplt.show()\n#summarize history for intersection over union\nplt.plot(history.history['mean_io_u'])\nplt.plot(history.history['val_mean_io_u'])\nplt.title('model mean_io_u')\nplt.ylabel('mean_io_u')\nplt.xlabel('epoch')\nplt.legend(['train','validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-20T20:59:23.787939Z","iopub.execute_input":"2022-01-20T20:59:23.788247Z","iopub.status.idle":"2022-01-20T20:59:24.495375Z","shell.execute_reply.started":"2022-01-20T20:59:23.788214Z","shell.execute_reply":"2022-01-20T20:59:24.494394Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test_dataset,steps=test_steps)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T20:57:56.092403Z","iopub.execute_input":"2022-01-20T20:57:56.093072Z","iopub.status.idle":"2022-01-20T20:57:58.339516Z","shell.execute_reply.started":"2022-01-20T20:57:56.093025Z","shell.execute_reply":"2022-01-20T20:57:58.338624Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}